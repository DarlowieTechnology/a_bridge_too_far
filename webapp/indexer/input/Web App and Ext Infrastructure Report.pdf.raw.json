{
  "SR-101-001": "SR-101-001 SQL Injection\nRisk Rating Critical Status Open\nDescription:\nA critical SQL injection vulnerability was found on one PHP page, allowing full access to the site\u2019s back-end database\nand extraction of sensitive customer details. This vulnerability occurs when user-supplied input is used in the dynamic\nconstruction of a SQL query, without sufficient input validation being performed. This is usually a very serious vulnerability, as it effectively allows a remote attacker to execute (often arbitrary) SQL commands on the underlying database\nserver with the privileges of the web application\u2019s database access, leaving the database open to execution of stored\nprocedures, privilege escalation, and information retrieval. It is important to note that this issue could be exploited by\nunauthenticated attackers.\nSQL injection vulnerabilities of this nature are frequently the cause of high-profile website breaches, so it is strongly\nrecommended that this issue is investigated further.\nThe vulnerability was identified in at least one of the PHP pages (/scabc/vulnerable_page.php, job parameter).\nThe results of an arbitrary query could be seen in the page by using the UNION operator to inject an additional SELECT\nquery into the job parameter of the URL. In the screenshot below, the query injected is select user():\nhttp://www.nccgroup-client.co.uk/scabc/vulnerable_page.php?job=171%20union%20select%201,2,3,user%28%29,\n5,6,7,8,9,10,11,12,13,14,15,16%20order%20by%201%20asc--&postcode=M1%207EF&r=1145767722\nData could be extracted efficiently using the MySQL LIMIT and OFFSET query options, obtaining one result row per\nHTTP request. For example, a query such as that shown below could be used to obtain the name of the first table in\nthe contact database (schema):\nhttp://www.nccgroup-client.co.uk/scabc/jo3.php?job=\n171union select 1,2,3,table_name,5,6,7,8,9,10,11,12,13,14,15,16 from information_schema.tables where table_schema = 0x6f6e65636f6e74616374 order by 1 asc limit 1 offset 1--\n&postcode=M1 7EF&r=1145767722\nThe Offset parameter would then be incremented to get the next row. The current database was identified as contact,\nwith contact_agent and contact_test also being present.\nVersion 3.0- Page 8 of 29 Client Confidential\nA list of tables for the current database was extracted as follows:\nFigure 1: Extracted information\nAll customer data (such as addresses and security questions and answers) could be eventually be extracted from the\ndatabase using this technique.\nRecommendation:\nThe breach was possible due to vulnerabilities in the PHP source code which queried the back-end database. While only a single parameter in one PHP page was identified as vulnerable, it is possible that further such vulnerabilities\nexist which were not discovered within the time constraints of the current black box test. The source code of all pages\nshould be reviewed for code patterns similar to those found in the vulnerable page, and all pages should be recoded\nto use prepared statements for database access.1, 2, 3\nThe same principle of using strict input validation and prepared statements applies equally to new development in\nother web frameworks such as ASP or ASP.NET. Banning particular SQL keywords in input as a fix for SQL injection is discouraged as this is often ineffective and can be evaded.\nFurther measures should be taken as part of a defence-in-depth strategy. Passwords should be stored as salted hashes\nand sensitive details (such as security questions/answers) should be stored encrypted. Users should be prevented from\nsetting very weak passwords and be advised to not use the same passwords they use elsewhere.\nIn general, dynamic SQL should not be used within the application. Environments such as J2EE, ASP.NET, PHP, and Perl\nsupport the use of parameterised queries or prepared statements to ensure that the structure of the SQL statement\nis defined prior to entering user input.\nIf it is necessary to use dynamic SQL, user input should be validated and sanitised first. Numeric input should be\npassed through a numeric check, and string input should be fixed to escape the single quote (\u2018) character.\nAffects:\nIP Address DNS Name\n192.001.1.111 www.clientwebsite.com\n1OWASP Guidancehttps://www.owasp.org/index.php/SQL_Injection https://www.owasp.org/index.php/SQL_Injection_Prevention_Cheat_Sheet ht\ntps://www.owasp.org/index.php/Query_Parameterization_Cheat_Sheet\n2OWASP Top 10\u2012Injection https://owasp.org/www-project-top-ten/2017/A1_2017-Injection\n3CWE-089: Improper Neutralization of Special Elements used in an SQL Command (\u2018SQL Injection\u2019)https://cwe.mitre.org/data/definitions/\n89.html\nVersion 3.0- Page 9 of 29 Client Confidential\n",
  "SR-101-002": "SR-101-002 Uploaded File Types Were Not Restricted\nRisk Rating High Status Open\nDescription:\nThe application allowed any type of file to be uploaded, without performing anti-virus scanning or other content\nchecking against the uploaded file. This meant that attackers could target other users of the application by uploading\nmalware to the server. They might also have been able target the server itself, by uploading a web shell that would\nprovide the attacker with the capability to run commands on the server and access the file system with the full privileges\nof the web server process.\nIt was also possible for an attacker to upload a malicious executable to the server. As a proof of concept, a web shell\nwas uploaded to the server. Weaknesses in the manner in which these files were stored meant it was then possible to\nexecute the web shell under the context of the user logged into the application which allowed some limited command\nexecution. More generally, files could also be downloaded and so might be run by other regular users which could,\nfor instance, facilitate the spread of malware.\nIt was also possible to upload a Flash file with an allowed extension such as .jpg; such a file could be used in a cross-site\ndata hijacking attack.\nFigure 2: Flash file with amended file extension accepted for upload\nIt was also found that uploaded files were robustly scanned for malware. As a proof of concept, the EICAR file was\nuploaded. This is a benign file used to test the response of anti-virus software. The application allowed the upload of\nthis file without any apparent server-side restriction, scanning, or removal of the file.\nRecommendation:\nThe file types allowed to be uploaded should be restricted to only those that are necessary for business functionality.4, 5, 6, 7, 8\nThe application should perform filtering and content checking on any files which are uploaded to the server. Files\nshould be thoroughly scanned and validated before being made available to other users. If in doubt, the file should\nbe discarded.\n4OWASP Guidancehttps://www.owasp.org/index.php/Unrestricted_File_Upload\n5EICAR Test Filehttp://www.eicar.org/86-0-Intended-use.html\n6CWE-434: Unrestricted Upload of File with Dangerous Typehttps://cwe.mitre.org/data/definitions/434.html\n7SANS \u20128 Basic Rules to Implement Secure File Uploadshttp://software-security.sans.org/blog/2009/12/28/8-basic-rules-to-implement-secure-\nfile-uploads/\n8The Pitfalls of Allowing File Uploads on Your Websitehttp://blog.detectify.com/post/86298380233/the-pitfalls-of-allowing-file-uploads-on-your-\nwebsite\nVersion 3.0- Page 10 of 29 Client Confidential\nAn anti-virus solution should be used to check the uploaded file; any files flagged as potentially malicious should be\ndiscarded.\nNote that the effectiveness of content-checking controls should not be wholly relied upon, and that they will only\nprovide benefit if the anti-virus and malware signatures are regularly updated.\nSome consideration should be given to storing uploaded files in a database, rather than on the file system. This would\nsignificantly reduce the risk associated with the file upload facility, but it is recognised that this might require extensive\nchanges to the current application design.\nWrite permission should be removed from files and folders other than the upload folders, if these are accessible to\nthe application. In IIS7 or higher, it is a good practice to disable or remove the dynamic extensions from the upload\nfolders by using the \u201cHandler Mappings\u201d section.\nAdding a \u201cContent-Disposition: Attachment\u201d header to static files such as PDF and document files will secure the\nwebsite against Flash-based cross-site data hijacking. This can be done by using \u201cHTTP Response Headers\u201d in IIS for\nthe upload folders.\nAffects:\nIP Address DNS Name\n192.001.1.111 www.clientwebsite.com\nVersion 3.0- Page 11 of 29 Client Confidential\n",
  "SR-101-003": "SR-101-003 Reflected Cross-Site Scripting\nRisk Rating Medium Status Open\nDescription:\nThe application was vulnerable to reflected or non-persistent cross-site scripting (XSS) attacks. This type of vulnerability\noccurs when data provided by a web client is used immediately by server-side scripts to generate a page of results for\nthe user. If unvalidated user-supplied data is included in the resulting page without full and proper HTML escaping,\nclient-side executable code may be injected into the dynamic page.\nIn the case of a GET request, this means that a URL which appears to be associated with the site (and therefore\ntrustworthy to regular users) could contain malicious code that would be executed by the user\u2019s browser within the\ncontext of the application when the link is visited. In the case of a POST request, a victim user would have to first be\ncoerced to an otherwise unrelated site which then launches attack using a form.\nFigure 3: XSS payload (in this case, a simple message) being executed under user\u2019s context\nEvidence of affected URLs, parameters, requests and responses has been redacted.\nReflected cross-site scripting vulnerabilities are extremely common in web applications but can have a serious impact.\nThey are typically used to launch site impersonation or phishing attacks, in which unsuspecting users are lured to\nmalicious sites via links that appear legitimate. The attacker is then free to present the user with what appears to\nbe genuine content, in an attempt, for example, to capture authentication credentials. Another common method of\nexploitation is to capture the session token of the victim user, allowing their session to be hijacked by the attacker.\nRecommendation:\nReliable avoidance of cross-site scripting vulnerabilities should consist of two stages - input validation and output\nencoding.9, 10, 11\nInput validation involves the application rejecting any characters which are invalid for the field in question, preferably\nby whitelisting a limited set of characters (in a telephone number field, for example, the whitelisted characters could\nbe 0-9, parentheses, and hyphens). This strategy can also help in mitigating other flaws which stem from a failure to\nsanitise input, such as SQL or HTTP header injection attacks.\nOutput encoding requires the encoding of all special characters (such as those used in HTML and JavaScript) in\n9OWASP XSS Referenceshttps://www.owasp.org/index.php/Cross-site_Scripting_(XSS) https://www.owasp.org/index.php/XSS_(Cross_Site_Scriptin\ng)_Prevention_Cheat_Sheet\n10OWASP Top 10\u2012Cross-Site Scriptinghttps://owasp.org/www-project-top-ten/2017/A72017-Cross-Site_Scripting(XSS)\n11CWE-079: Improper Neutralization of Input During Web Page Generation (\u2018Cross-site Scripting\u2019)https://cwe.mitre.org/data/definitions/\n79.html\nVersion 3.0- Page 12 of 29 Client Confidential\npotentially malicious data. This is generally done directly before display by web applications (or client-side script), and\nmany programming languages have built-in functions or libraries which provide this encoding (also called quoting or\nescaping in this context). Note that the correct encoding of the output depends on the location that the data is to\nbe used within the response. In the case of it being within the main body of the document, HTML entities must be\nencoded. If the input is to be used within a script inside of a string, the quotes used for that string must be escaped.\nIn general, it is important to ensure that it is not possible for the data to include whatever sequence is used to demark the end of that data and the beginning of something else.\nThe pages listed should be modified to handle malicious data properly in the associated fields.\nAffects:\nIP Address DNS Name Page Parameters\n192.001.1.111 www.clientwebsite.com vulnerablepage.html aed222, aed223\nVersion 3.0- Page 13 of 29 Client Confidential\n",
  "SR-101-007": "SR-101-007 Login to Administrative Interface Exposed\nRisk Rating Medium Status Open\nDescription:\nA number of login pages for accessing administrative interfaces were identified during testing of the external infrastructure. This is against security best practice which recommends that login pages to access such interfaces should\nbe only available to trusted IP addresses.\nThe following login pages were identified:\nNccgroup-client.co.uk/login\nRecommendation:\nImplement access control lists which only allow access from trusted IP addresses.\nAffects:\nIP Address DNS Name\n192.001.1.111 www.clientwebsite.com\nVersion 3.0- Page 14 of 29 Client Confidential\n",
  "SR-101-004": "SR-101-004 ASP.NET Header Disclosure\nRisk Rating Low Status Open\nDescription:\nVarious headers produced by the application provide information about the software installed on the web server. An\nattacker may use this information to gain a greater understanding of the underlying technologies involved and tailor\nfurther attacks to these specific products. It is therefore good practice to exclude information such as this from HTTP\nresponses.\nAn example HTTP response from the server is shown below:\nFigure 4: HTTP response\nThe highlighted headers revealed the exact version of ASP.NET which is in use.\nRecommendation:\nThe web server should be reconfigured so that software version information is not included in HTTP responses.12, 13, 14, 15, 16\nu For IIS 7.5 and later, the URL Rewrite HTTP module available from Microsoft can be configured to remove the Server\nheader from IIS responses.\nu The X-Powered-By header can be removed via the \u201cCustom HTTP Headers\u201d section of IIS Manager for the relevant\nsite.\nu The X-AspNet-Version header can be removed by adding the following node to the\n<httpRuntime enableVersionHeader=\"false\" />\nu The X-AspNetMvc-Version header can be removed by adding the following line of code to the ApplicationStart\nmethod of the application\u2019s Global.asax.cs file:\nMvcHandler.DisableMvcResponseHeader = true;\n12CWE-200: Information Exposure: https://cwe.mitre.org/data/definitions/200.html\n13MSDN - Remove Unwanted HTTP Response Headers: http://blogs.msdn.com/b/varunm/archive/2013/04/23/remove-unwanted-http-response-\nheaders.aspx\n14Removing Unnecessary HTTP Headers in IIS and ASP.NET: http://www.4guysfromrolla.com/articles/120209-1.aspx\n15OWASP Examples: https://www.owasp.org/index.php/Testing_for_Web_Application_Fingerprint_(OWASP-IG-004)\n16Change or modify a Response Header value using URL Rewrite: http://blogs.msdn.com/b/benjaminperkins/archive/2012/11/02/change-or-\nmodify-a-response-header-value-using-url-rewrite.aspx\nVersion 3.0- Page 15 of 29 Client Confidential\nAffects:\nIP Address DNS Name\n192.001.1.111 www.clientwebsite.com\nVersion 3.0- Page 16 of 29 Client Confidential\n",
  "SR-101-005": "SR-101-005 HTTP Header Discloses Internal IP Address\nRisk Rating Low Status Open\nDescription:\nIt was possible to determine an internal IP address by sending a crafted request to the application. An attacker may\nuse this information to gain a greater understanding of the internal network and tailor further attacks.\nThe HTTP request and response shown below illustrate this issue.\nFigure 5: HTTP response\nRecommendation:\nOn IIS version 7 and above, this issue can be addressed by setting the IIS alternateHostname property (as described\nat the www.iis.net link below). This allows the specified hostname to be used in place of the internal IP address\nin redirection responses. Testing should be performed to ensure that this does not adversely affect any legitimate\nredirection functionality in the application.17, 18, 19\nAffects:\nIP Address DNS Name\n192.001.1.111 www.clientwebsite.com\n17CWE-200: Information Exposure:https://cwe.mitre.org/data/definitions/200.html\n18Server Runtime <serverRuntime>:http://www.iis.net/configreference/system.webserver/serverruntime\n19Microsoft Support\u2012FIX: http://support.microsoft.com/kb/834141\nVersion 3.0- Page 17 of 29 Client Confidential\nSR-101-009 WSDL Descriptions Could Be Obtained By Unauthenticated Users\nRisk Rating Low Status Open\nDescription:\nIt was possible to retrieve Web Services Description Language (WSDL) from web service endpoints as an anonymous\nuser. Whilst this functionality could be of use to a legitimate developer, it would also help an attacker to determine the\nmethods exposed by a service and how to create a well-formed request.\nFigure 6: WSDL file\nRecommendation:\nFor WCF services, the displaying of exception details is largely controlled with the \u201cserviceMetadata\u201d property, which is\nset in the web.config file. To prevent the download of WSDL descriptions, the configuration directive should be set as\nshown:20, 21, 22\n<serviceDebug includeExceptionDetailInFaults=\"false\"/>\nAlternatively, the URLScan tool, the IIS Request Filtering feature, or a web application firewall could be used to block\nincoming requests for WSDL descriptions.\nAffects:\nIP Address DNS Name\n192.001.1.111 www.clientwebsite.com\n20<serviceMetadata> behaviour:http://msdn.microsoft.com/en-us/library/ms731317(v=vs.110).aspx\n21IIS Request Filtering:http://www.iis.net/configreference/system.webserver/security/requestfiltering\n22Microsoft\u2019s Free Security Tools\u2012URLScan Security Tool:http://blogs.technet.com/b/security/archive/2013/01/22/microsoft-s-free-security-tools-\nurlscan-security-tool.aspx\nVersion 3.0- Page 18 of 29 Client Confidential\n",
  "SR-101-010": "SR-101-010 Invalid SSL Certificate\nRisk Rating Low Status Open\nDescription:\nThe SSL certificate installed on the service was invalid, and would not be trusted by connecting applications. This would\nmake a man-in-the-middle attack easier to perform.\nThe certificate was invalid because:\nu It was self-signed\n\u2013 The endpoint cannot be validated because the certificate has not been issued by a recognised certificate authority\nA screenshot showing the certificate and highlighting the relevant detail can be seen inSupplemental Data - SSL\nCertificate Details on page 26.\nAn end user attempting to contact the secure service would likely be presented with a certificate warning, which to\nmost users would be indistinguishable from the warning associated with a fraudulent certificate. The chance of a\nsuccessful man-in-the-middle attack against the remote host is therefore increased. Such an attack would allow the\nattacker to view and edit data in transit without the knowledge of the end user.\nIn addition, a certificate warning being issued by the browser could reduce user confidence in the site\u2019s security.\nRecommendation:\nA new SSL certificate should be purchased or generated to replace the existing one. The certificate should be signed\nby an established certificate authority and cover all the hosts on which it will be installed (preferably using Subject\nAlternative Names where multiple hostnames are required).23, 24, 25, 26\nAffects:\nIP Address DNS Name\n192.001.1.111 www.clientwebsite.com\n23Crying Wolf: An Empirical Study of SSL Warning Effectiveness:http://lorrie.cranor.org/pubs/sslwarnings.pdf\n24Benign security warnings have trained users to ignore them:http://arstechnica.com/security/2009/07/benign-security-warnings-have-trained-u\nsers-to-ignore-them/\n25Alice in Warningland: A Large-Scale Field Study of Browser Security Warning Effectiveness:https://www.usenix.org/conference/usenixsecurity13\n/technical-sessions/presentation/akhawe\n26NCC Group whitepaper on the configuration of SSL/TLS services:https://www.nccgroup.trust/en/learning-and-research-centre/white-papers/ho\nw-organisations-can-properly-configure-ssl-services-to-ensure-the-integrity-and-confidentiality-of-data-in-transit/\nVersion 3.0- Page 19 of 29 Client Confidential\n",
  "SR-101-012": "SR-101-012 Hosts Respond to ICMP\nRisk Rating Low Status Open\nDescription:\nHosts were found to respond to ICMP messages. Further information has been included inSupplemental Data - ICMP\nDetails on page 27.\nRecommendation:\nAll inbound and outbound ICMP messages should be restricted at border routers and firewall devices; if they are\nspecifically required for legitimate business reasons then access control lists for these services should be applied.\nNote that completely disabling ICMP may restrict PMTU discovery and might affect other applications on the network.\nThe ICMP requirements for the network should be considered before implementing any changes to filtering controls.\nAffects:\nIP Address DNS Name\n192.001.1.111 www.clientwebsite.com\nVersion 3.0- Page 20 of 29 Client Confidential\n",
  "SR-101-013": "SR-101-013 Directory Listings Enabled\nRisk Rating Low Status Open\nDescription:\nDirectory listings were enabled for the site. This allows an attacker to browse directories for which there is no index\npage (e.g. index.html, index.php, or default.aspx) present.\nThis can be used to gain knowledge of the file and folder structure, and makes it easy to look for information that may\nbe useful when carrying out a targeted attack, such as the specific frameworks or libraries in use. It may also lead to\nthe discovery of sensitive files which, without directory listings, would be much harder to find by brute-force.\nFigure 7: Screenshot of file structure\nRecommendation:\nDirectory listings should be disabled.27, 28\nFor IIS, this can be achieved in IIS Manager via the Directory Browsing section - the \u201cDisable\u201d option should be selected. Alternatively, the change can be carried out by direct modification of the relevant web.config file, in the\nsystem.webServer/directoryBrowse attribute.29\nAffects:\nIP Address DNS Name\n192.001.1.111 www.clientwebsite.com\n27CWE-548: Information Exposure Through Directory Listing:http://cwe.mitre.org/data/definitions/548.html\n28Enable or Disable Directory Browsing in IIS 7:http://technet.microsoft.com/en-us/library/cc731109(WS.10).aspx http://technet.microsoft.com/en-\nus/library/cc731109(WS.10).aspx\n29Directory Browse setting:http://www.iis.net/configreference/system.webserver/directorybrowse\nVersion 3.0- Page 21 of 29 Client Confidential\n",
  "SR-101-015": "SR-101-015 No Support for TLS Versions Above 1.0\nRisk Rating Low Status Open\nDescription:\nThe affected server did not support TLS above version 1.0. Since 2008, TLS has been at version 1.2, with version 1.3\nbeing defined in August 2018. Versions 1.2 and 1.3 of TLS are more resistant to known attacks, and TLSv1.2 supports\nmore modern cipher suites that are widely acknowledged to offer the best cryptography available for securing Internet\nconnections. TLSv1.3 further improves on security by removing unsafe or unused features, eliminating unnecessary\nhandshake steps and forcing the use of newer encryption methods. t should be noted that use of TLS 1.0 and 1.1 is\nnow flagged as insecure (such as by browser warnings) by the major browsers, and in January 2021 the NSA issued\nguidance urging system administrators to only provide support for TLS versions 1.2 or 1.3.30\nTLS 1.0 was the highest protocol version supported, as shown inSupplemental Data - SSLScan Output on page 25.\nRecent versions of all web browsers support 1.2 and 1.3, and therefore supporting these protocols server-side offers\nbetter security for those clients using modern browsers. TLSv1.2 also supports a class of cipher suites that offer\nAuthenticated Encryption with Associated Data (AEAD). These cipher suites include an authenticated integrity check\nwithin the encryption operation, and are resistant to more attacks than their older counterparts. Google\u2019s Chrome\nbrowser states that the security of connections to websites that do not use AEAD ciphers is \u201cobsolete\u201d. Currently, this\nis not presented as an error to the user, merely as information for anyone viewing the page\u2019s security report (in the\nbrowser\u2019s developer console). However, as Google continues to push for the rapid adoption of more robust Internet\ncryptography, this message may become more prominent.\nTLSv1.3 was approved for use in August 2018 and goes to further enhance security. The security benefits of TLSv1.3\ninclude the removal of unsafe or unused features, elimination of unnecessary handshake steps and the forced use of\nnewer encryption methods. As well as preventing encryption downgrade attacks, the streamlined approach to session\ninitiation will offer performance gains over previous TLS implementations.\nSupporting TLS 1.0 as a security control is non-compliant with the Payment Card Industry (PCI) Data Security Standard\n(DSS). While these regulations may not be directly relevant, it should be expected that this directive will become security\nbest practice and may also be reflected in more obvious ways, such as web browser warnings.\nRefer also to the finding SR-101-014 on page 24.\nRecommendation:\nAdd support for TLS v1.2 and v1.3.31, 32, 33, 34, 35, 36, 37\nSupport for TLS version 1.0 should be disabled unless there is a specific business requirement to allow users of older,\nless secure browsers to continue to connect. Consider also disabling support for version 1.1, which will become\nunsupported by major browsers in the near future.\nAll affected hosts should be configured to prefer the latest cipher suites that they offer, such as AES-GCM (TLS 1.2 and\n1.3 only).\nThe reference from Mozilla below provides recommendations on cipher suite ordering based on the profile of connecting clients.\n30Eliminating Obsolete Transport Layer Security (TLS) Protocol Configurations -https://media.defense.gov/2021/Jan/05/2002560140/-1/-1/0/ELIMI\nNATING_OBSOLETE_TLS_UOO197443-20.PDF\n31Mozilla Server Side TLS:https://wiki.mozilla.org/Security/Server_Side_TLS#Recommended_configurations\n32Obsolete Cipher Suites:https://www.chromium.org/Home/chromium-security/education/tls#TOC-Obsolete-Cipher-Suites\n33OWASP Transport Layer Protection Cheat Sheet:https://cheatsheetseries.owasp.org/cheatsheets/Transport_Layer_Protection_Cheat_Sheet.html\n34NCC Group Whitepaper on the Configuration of SSL/TLS Services:www.nccgroup.com\n35PCI Security Standards Council:https://www.pcisecuritystandards.org/documents/Migrating-from-SSL-Early-TLS-Info-Supp-v1_1.pdf\n36Saying Goodbye to SSL/Early TLS:blog.pcisecuritystandards.org\n37Overview of TLS v1.3:https://owasp.org/www-pdf-archive/OWASPLondon20180125_TLSv1.3_Andy_Brodie.pdf\nVersion 3.0- Page 22 of 29 Client Confidential\nAffects:\nIP Address DNS Name\n192.001.1.111 www.clientwebsite.com\nVersion 3.0- Page 23 of 29 Client Confidential\n",
  "SR-101-014": "SR-101-014 BEAST SSL / TLS Weaknesses\nRisk Rating Informational Status Open\nDescription:\nA vulnerability that could allow information disclosure exists in SSL and version 1.0 of TLS. The weakness is caused by an\nimproper choice of initialisation vector (IV) used by block ciphers operating in cipher block chaining (CBC) mode. The\nexploit that takes advantage of the vulnerability is known as \u201cbrowser exploit against SSL/TLS\u201d (BEAST). BEAST allows\nattackers to compromise the confidentiality of connections to reveal short sections of plaintext, with session cookies\nbeing the most likely target. The potential for the BEAST vulnerability has been reported here because the servers\nwere seen to support block ciphers in CBC mode operating under vulnerable SSL/TLS protocol versions.\nThe BEAST attack itself is a client-side attack and requires the attacker to not only be in a position to inspect the\nencrypted traffic but also to initiate crafted requests made from the victim\u2019s browser. In addition to this requirement,\nthe major browser vendors have implemented client-side fixes for the IV flaw (Apple being the last to do so in October\n2013) and thus users with up-to-date browsers should not be affected. For these reasons the issue has been rated as\ninformational.\nRecommendation:\nNo server-side remedial action can fully eliminate the conditions necessary for a successful BEAST attack. While\ndisabling SSL is recommended, removing support for TLS version 1.0 could prevent some users from accessing the\nservice.38, 39, 40\nWhile prioritising the use of cipher suites based on the RC4 stream cipher would mitigate BEAST, RC4 has been shown\nto suffer from cryptographic flaws that mean this action is not a recommended solution.\nTLS versions 1.1 and 1.2 are not susceptible to the weak IV design. However, while supporting them is recommended,\nthat does not specifically resolve the BEAST attack because the man-in-the-middle attacker can launch what is known\nas a \u201cprotocol downgrade attack\u201d. In this attack the man-in-the-middle interferes with the TLS connection to try to force\nbrowsers to use lower TLS versions that are vulnerable to BEAST. This downgrade attack can be mitigated by supporting\nthe TLS_FALLBACK_SCSV mechanism, but this must also be supported by the user\u2019s browser for the mitigation to work.\nAffects:\nIP Address DNS Name\n192.001.1.111 www.clientwebsite.com\n38Original BEAST Attack:http://vnhacker.blogspot.co.uk/2011/09/beast.html\n39NCC Group Whitepaper on the Configuration of SSL/TLS Services:https://www.nccgroup.trust/en/learning-and-research-centre/white-papers/ho\nw-organisations-can-properly-configure-ssl-services-to-ensure-the-integrity-and-confidentiality-of-data-in-transit/\n40SSL Labs:https://community.qualys.com/blogs/securitylabs/2013/09/10/is-beast-still-a-threat https://community.qualys.com/blogs/securitylabs/\n2013/10/31/apple-enabled-beast-mitigations-in-os-x-109-mavericks\nVersion 3.0- Page 24 of 29 Client Confidential\n"
}